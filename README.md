# k8s_test_sre
test_k8s
!WARNING! данное решение приведено только в ознакомительных целях.

![изображение](https://github.com/F-1X/k8s_test_sre/assets/73891028/503fa07d-b062-4ec1-b635-9764be2a94ea)


livenessProbe установлена так: 5 секунд перед проведением первой пробы (запуск приложения), затем каждые 5 секунд. Таким образом промежуток 5-10 секунд покрывается проверкой.
initialDelaySeconds: 5
periodSeconds: 5

resources - на первые запросы требуется больше ресурсов, чем впоследстии. Память установим в 128M (думаю лучше добавить 1-2% от возможной погрешности, всплесков, итого 130М. Или завысить лимиты в х2, так-же возможно испольвание VPA, для масштабирования ресурсов, если позволяет конкекст условной задачи).
Т.к. мы установили replicas=2 в деплойменте, задержка hpa позволяет отложить уменьшение реплик на 300 секунд (можно переопределить через behavior).

Масштабирование с использованием CronJob позволяет ограничить кол-во реплик до 1 с 23:00 часов до 9 утра. Метод не самый надежный или корректный т.к. не учитывается текущая нагрузка на поды.

Мультизональность и отказоустойчиность.
affinity поды будут распределяться равномерно в зоне покрытия topologyKey: topology.kubernetes.io/zone

HPA
масштабирование реплик от 1 до 4. behavior оставил дефолтным
averageUtilization: 85 - расчитывается как среднее абсолютное со всех подов к желанному(сумарный request c подов). Выделение новых подов будет отношением вышеприведенного исчисления к требуемому averageUtilization 0.85
averageValue: 128M

ЗЫ: ServiceAccount нужен для доступа CronJob к деплойменту (вызовы API). Ingress controller не приведен в данном примере.
